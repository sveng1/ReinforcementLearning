{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_probs(Q,state,epsilon):\n",
    "    #returns probability of each action for a given state\n",
    "    probs = np.zeros((Q.shape[1]))+epsilon/Q.shape[1]\n",
    "    best_action = Q[state,:].argmax()\n",
    "    probs[best_action] += 1 - epsilon\n",
    "    return probs\n",
    "\n",
    "def choose_action(pi_probs,n_actions):\n",
    "    #returns action based on policy\n",
    "    return np.random.choice(range(n_actions),p=pi_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env,n_episodes,alpha,gamma,epsilon):\n",
    "    \n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    #initialize action-value function\n",
    "    Q = np.zeros((n_states,n_actions)) + 0.5\n",
    "    Q[n_states-1,:] = np.zeros(n_actions)\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        next_state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        #generate episode\n",
    "        while not done:\n",
    "            \n",
    "            state = next_state\n",
    "            action = choose_action(epsilon_greedy_probs(Q,state,epsilon),n_actions)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            probs = epsilon_greedy_probs(Q,next_state,epsilon)\n",
    "            \n",
    "            #TD update of action-value function\n",
    "            Q[state,action] += alpha * (reward + gamma * np.sum(probs[a]*Q[next_state,a] for a in range(n_actions)) - Q[state,action])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = expected_sarsa(env,2000,0.8,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06512623, 0.11657942, 0.09439198, 0.06617008],\n",
       "       [0.09859404, 0.1008878 , 0.10385653, 0.08419835],\n",
       "       [0.10894218, 0.1221738 , 0.13388931, 0.12379043],\n",
       "       [0.10459233, 0.12730272, 0.15955584, 0.10415731],\n",
       "       [0.09353305, 0.26161564, 0.09720503, 0.09848714],\n",
       "       [0.10516636, 0.11171306, 0.23037542, 0.1005726 ],\n",
       "       [0.10700139, 0.10690943, 0.12162299, 0.11121538],\n",
       "       [0.11814847, 0.13298698, 0.11114836, 0.09825464],\n",
       "       [0.09947446, 0.10434635, 0.11841665, 0.08040223],\n",
       "       [0.09006615, 0.091728  , 0.26154873, 0.09508759],\n",
       "       [0.14688228, 0.11301591, 0.17827709, 0.12378641],\n",
       "       [0.1982952 , 0.22846081, 0.18777419, 0.13457142],\n",
       "       [0.36612532, 0.12398119, 0.14138722, 0.10475301],\n",
       "       [0.11458951, 0.22587756, 0.10757546, 0.12583371],\n",
       "       [0.13517924, 0.19125275, 0.11952932, 0.11573972],\n",
       "       [0.1465823 , 0.12926727, 0.13232908, 0.11861467],\n",
       "       [0.11227528, 0.09786017, 0.13970567, 0.09984749],\n",
       "       [0.12623921, 0.23383441, 0.09904989, 0.10379787],\n",
       "       [0.14497425, 0.2701849 , 0.42555188, 0.16903332],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.15780798, 0.39738535, 0.17222861, 0.24340909],\n",
       "       [0.24146896, 0.19304528, 0.20831516, 0.21846216],\n",
       "       [0.26577378, 0.16468237, 0.16170605, 0.14382463],\n",
       "       [0.1453072 , 0.15998313, 0.13944665, 0.14542465],\n",
       "       [0.11972843, 0.30187043, 0.12970848, 0.11253041],\n",
       "       [0.14941594, 0.1471236 , 0.28437358, 0.16837773],\n",
       "       [0.1873448 , 0.26356377, 0.29724316, 0.16744341],\n",
       "       [0.31489433, 0.30059096, 0.45      , 0.26330972],\n",
       "       [0.26616943, 0.34726478, 0.23507343, 0.44422522],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.33016068, 0.27388163, 0.25278775, 0.27361881],\n",
       "       [0.2076212 , 0.3189405 , 0.21576899, 0.20702997],\n",
       "       [0.26305057, 0.38160085, 0.1670868 , 0.19025426],\n",
       "       [0.16812884, 0.29286956, 0.42616726, 0.18985447],\n",
       "       [0.22493082, 0.30709479, 0.43212089, 0.22290239],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.40941534, 0.35213393, 0.3194537 , 0.34215784],\n",
       "       [0.39169957, 0.33173534, 0.37331754, 0.37540398],\n",
       "       [0.35516376, 0.33782363, 0.34051529, 0.34490949],\n",
       "       [0.30828143, 0.29818992, 0.29362272, 0.38015676],\n",
       "       [0.26992492, 0.25658745, 0.44873391, 0.24998456],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.46      , 0.46      , 0.42261852, 0.42210557],\n",
       "       [0.44997683, 0.40093892, 0.36841564, 0.42185956],\n",
       "       [0.39344337, 0.44970297, 0.39418968, 0.38521676],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.34108721, 0.37321801, 0.36074285, 0.43415972],\n",
       "       [0.24203127, 0.44884014, 0.24395985, 0.24439176],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.40850689, 0.38838246, 0.46      , 0.45924112],\n",
       "       [0.45784   , 0.43118634, 0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.42317408, 0.452     , 0.45156282, 0.43034341],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.46097661, 0.9       , 0.42618176, 0.5       ],\n",
       "       [0.29501572, 0.29664766, 0.27788935, 0.28239238],\n",
       "       [0.30130875, 0.29213643, 0.27993321, 0.44128661],\n",
       "       [0.35200854, 0.36906699, 0.44992761, 0.37871374],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.42196397, 0.45928   , 0.45928   , 0.46      ],\n",
       "       [0.46      , 0.5       , 0.5       , 0.5       ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
