{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('FrozenLake8x8-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_agent:\n",
    "    def __init__(self):\n",
    "        print(\"Constructing MC agent\")\n",
    "        self.Q = None\n",
    "    \n",
    "    def random_policy(self,state,n_actions):\n",
    "        return np.random.randint(n_actions)\n",
    "    \n",
    "    def epsilon_greedy_policy(self,Q,state,n_actions,epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(n_actions)\n",
    "        else:\n",
    "            return Q[state,:].argmax()\n",
    "        \n",
    "    def greedy_policy(self,Q,state):\n",
    "        return Q[state,:].argmax()\n",
    "    \n",
    "    def predict(self,env,n_episodes,gamma,policy=False):\n",
    "        \"\"\"\n",
    "        Estimates value function for given policy\n",
    "        Parameters\n",
    "            env: the environment e.g. gridworld, frozen-lake\n",
    "            policy: the policy to be evaluated (parameters: (state, n_actions), returns: (action))\n",
    "            gamma: the discount factor\n",
    "        Returns\n",
    "            V: state-value function\n",
    "        \"\"\"\n",
    "        n_states = env.observation_space.n\n",
    "        n_actions = env.action_space.n\n",
    "    \n",
    "        #initialize state-value function as zeros\n",
    "        self.V = np.zeros((n_states))\n",
    "        #initialize state visit counter\n",
    "        N = np.zeros((n_states))\n",
    "    \n",
    "        for episodes in range(n_episodes):\n",
    "        \n",
    "            observation = env.reset()\n",
    "            state_reward_pairs = []\n",
    "        \n",
    "            #generate episode of 200 time steps\n",
    "            for t in range(200):\n",
    "                \n",
    "                #get action from policy\n",
    "                if not policy:\n",
    "                    action = self.random_policy(observation,n_actions)\n",
    "                old_observation = observation\n",
    "                \n",
    "                #take action \n",
    "                observation, reward, done, info = env.step(action)\n",
    "                \n",
    "                #save state and reward\n",
    "                state_reward_pairs.append((old_observation,reward))\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            #update value function for visited states\n",
    "            G = 0\n",
    "            for state,reward in state_reward_pairs[::-1]:\n",
    "                G = reward + gamma * G\n",
    "                N[state] += 1\n",
    "                V[state] += (G - V[state])/N[state]\n",
    "                \n",
    "        return V\n",
    "    \n",
    "    \n",
    "    def control(self,env,n_episodes,gamma):\n",
    "        \"\"\"\n",
    "        On-policy control\n",
    "        Estimates optimal action-value function for environment\n",
    "        Parameters\n",
    "            env: the environment e.g. gridworld, frozen-lake\n",
    "            gamma: the discount factor\n",
    "        Returns\n",
    "            Q: action-value function\n",
    "        \"\"\"\n",
    "        n_states = env.observation_space.n\n",
    "        n_actions = env.action_space.n\n",
    "    \n",
    "        #initialize action-value function\n",
    "        Q = np.zeros((n_states,n_actions))\n",
    "        #initialize state-action visit counter\n",
    "        N = np.zeros((n_states,n_actions))\n",
    "        \n",
    "        for i_episode in range(n_episodes):\n",
    "            \n",
    "            observation = env.reset()\n",
    "            state_action_reward_tuples = []\n",
    "            \n",
    "            for t in range(200):\n",
    "                action = self.random_policy(observation,n_actions)\n",
    "                old_observation = observation\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state_action_reward_tuples.append((old_observation, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            #update action-value function\n",
    "            G = 0\n",
    "            for state,action,reward in state_action_reward_tuples[::-1]:\n",
    "                G = reward + gamma * G\n",
    "                N[state,action] += 1\n",
    "                Q[state,action] += (G - Q[state,action])/N[state,action]\n",
    "        self.Q = Q\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing MC agent\n"
     ]
    }
   ],
   "source": [
    "agent = MC_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.greedy_policy(Q,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = agent.predict(env,1000,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = agent.control(env,20000,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.88963546e-05, 1.93551734e-04, 2.37534655e-04, 7.80606406e-05])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAACmNJREFUeJzt3V+IXPUZxvHncTXYVG0u2oaQhCYF\nyUULJRIsYhCqWNIq1YteRFCoFHKlKL0Q7V0vSu/EXgkhaoWmSokKIq2poNYWWptNTKn5o6TBNptq\nohTrn0DWbJ5e7ARiiZmzmXNmzr79fiC4uzk78w7yzTkzc+Z3nEQAarpo0gMA6A6BA4UROFAYgQOF\nEThQGIEDhRE4UBiBA4UROFDYxV3cqG1OjwM6lsTDtukkcKAv7KENLEpNTzHnEB0ojMCBwggcKIzA\ngcIIHCiMwIHCCBwojMCBwhoFbnuT7TdsH7J9f9dDAWiHh50RY3tK0puSbpQ0I2mXpNuS7D/P73Cq\nKnqh8plsTU5VbbIHv1rSoSSHk8xKelLSLaMOCKB7TQJfKenIWd/PDH4GoOda+7CJ7S2StrR1ewBG\n1yTwo5JWn/X9qsHPPiXJVklbJZ6DA33R5BB9l6Qrba+1vUTSZknPdjsWgDYM3YMnOWX7Lkk7JU1J\nejTJvs4nAzCyoW+TXdCNcoiOnuBtMgBlEThQGIEDhRE4UBiBA4UROFAYgQOFEThQGFc2AVp00UXj\n2WfOzc012o49OFAYgQOFEThQGIEDhRE4UBiBA4UROFAYgQOFEThQ2NDAbT9q+7jt18cxEID2NNmD\n/0LSpo7nANCBoYEneUXSv8cwC4CW8RwcKIxLFwGFNVoX3fYaSc8l+XqjG2VddPTEuNdFH+fHRVkX\nHfg/1+Rtsick/UnSOtsztn/Y/VgA2sCli1Aah+gAyiJwoDACBwojcKAwAgcKI3CgMAIHCiNwoDAu\nXYTSxnXiybjv7/Tp0422Yw8OFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBhTRZdXG37\nJdv7be+zfc84BgMwuqGLLtpeIWlFkj22L5e0W9KtSfaf53dYdBG9MDU1Ndb7G9e56KdOndLp06dH\nX3QxydtJ9gy+/lDSAUkrRx8RQNcW9GmywRVO1kt69Rx/x6WLgJ5pvC667csk/V7ST5M8PWRbDtHR\nCxyiN2D7EklPSdo+LG4A/dHkVXRLekTSgSQPdj8SgLY02YNfK+kOSdfb3jv4892O5wLQAq5NhtJ4\nDg6gLAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCuPaZCht3Ce6LFmyZCz3Mzc312g79uBAYQQOFEbg\nQGEEDhRG4EBhBA4URuBAYQQOFEbgQGFNFl281PZfbP91cOmin4xjMACja3Kq6klJ1yf5aLB88h9t\n/zbJnzueDcCIhgae+VUZPxp8e8ngD4sqAotA0wsfTNneK+m4pBeSnPPSRbanbU+3PSSAC7OgZZNt\nL5P0jKS7k7x+nu3Yw6MXxvXprnHf34kTJzQ3N9fusslJ3pf0kqRNFzoYgPFp8ir6lwZ7btn+nKQb\nJR3sejAAo2vyKvoKSY/bntL8Pwi/TvJct2MBaAOXLkJpPAcHUBaBA4UROFAYgQOFEThQGIEDhRE4\nUBiBA4Vx6SKM3dKlS8d2X5988snY7kuSli1bNpb7OXnyZKPt2IMDhRE4UBiBA4UROFAYgQOFEThQ\nGIEDhRE4UBiBA4U1DnywNvprtlmPDVgkFrIHv0fSga4GAdC+plc2WSXpJknbuh0HQJua7sEfknSf\npNMdzgKgZU0ufHCzpONJdg/ZjmuTAT0zdF102z+TdIekU5IulXSFpKeT3H6e32FddHymyh8XXb58\n+Vju59ixY5qdnR19XfQkDyRZlWSNpM2SXjxf3AD6g/fBgcIWtKJLkpclvdzJJABaxx4cKIzAgcII\nHCiMwIHCCBwojMCBwggcKIzAgcK4dBHG7sSJE5MeoTMbN24cy/3s3Lmz0XbswYHCCBwojMCBwggc\nKIzAgcIIHCiMwIHCCBwojMCBwhqdyWb7LUkfSpqTdCrJhi6HAtCOhZyq+q0k73U2CYDWcYgOFNY0\n8Ej6ne3dtrd0ORCA9jQ9RN+Y5KjtL0t6wfbBJK+cvcEgfOIHeqTRHjzJ0cF/j0t6RtLV59hma5IN\nvAAH9EeTiw9+3vblZ76W9G1Jr3c9GIDRNTlEXy7pGdtntv9Vkuc7nQpAK4YGnuSwpG+MYRYALeNt\nMqAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKc5L2b9Ru/0aBRaCLns5lw4YNmp6e9rDt2IMDhRE4\nUBiBA4UROFAYgQOFEThQGIEDhRE4UBiBA4U1Ctz2Mts7bB+0fcD2NV0PBmB0TddF/7mk55N83/YS\nSUs7nAlAS4YGbvsLkq6T9ANJSjIrabbbsQC0ockh+lpJ70p6zPZrtrcN1kcH0HNNAr9Y0lWSHk6y\nXtLHku7/341sb7E9bXu65RkBXKAmgc9Imkny6uD7HZoP/lO4dBHQP0MDT/KOpCO21w1+dIOk/Z1O\nBaAVTV9Fv1vS9sEr6Icl3dndSADa0ijwJHslcegNLDKcyQYURuBAYQQOFEbgQGEEDhRG4EBhBA4U\nRuBAYQQOFNb0VFUADdhDLxc2VuzBgcIIHCiMwIHCCBwojMCBwggcKIzAgcIIHCiMwIHChgZue53t\nvWf9+cD2veMYDsBonKT5xvaUpKOSvpnkH+fZrvmNArggSYaeF7vQQ/QbJP39fHED6I+Ffthks6Qn\nzvUXtrdI2jLyRABa0/gQfXDRg39J+lqSY0O25RAd6Fjbh+jfkbRnWNwA+mMhgd+mzzg8B9BPjQ7R\nB9cD/6ekryb5T4PtOUQHOtbkEH1Bb5M1ReBA97p4mwzAIkLgQGEEDhRG4EBhBA4URuBAYQQOFEbg\nQGFdXbroPUkL/UjpFwe/V1HVx8bjmpyvNNmokzPZLoTt6SQbJj1HF6o+Nh5X/3GIDhRG4EBhfQp8\n66QH6FDVx8bj6rnePAcH0L4+7cEBtKwXgdveZPsN24ds3z/pedpge7Xtl2zvt73P9j2TnqlNtqds\nv2b7uUnP0ibby2zvsH3Q9gHb10x6plFM/BB9sNb6m5JulDQjaZek25Lsn+hgI7K9QtKKJHtsXy5p\nt6RbF/vjOsP2jyRtkHRFkpsnPU9bbD8u6Q9Jtg0WGl2a5P1Jz3Wh+rAHv1rSoSSHk8xKelLSLROe\naWRJ3k6yZ/D1h5IOSFo52anaYXuVpJskbZv0LG2y/QVJ10l6RJKSzC7muKV+BL5S0pGzvp9RkRDO\nsL1G0npJr052ktY8JOk+SacnPUjL1kp6V9Jjg6cf2wbrES5afQi8NNuXSXpK0r1JPpj0PKOyfbOk\n40l2T3qWDlws6SpJDydZL+ljSYv6NaE+BH5U0uqzvl81+NmiZ/sSzce9PcnTk56nJddK+p7ttzT/\ndOp627+c7EitmZE0k+TMkdYOzQe/aPUh8F2SrrS9dvCixmZJz054ppHZtuafyx1I8uCk52lLkgeS\nrEqyRvP/r15McvuEx2pFknckHbG9bvCjGyQt6hdFu/o0WWNJTtm+S9JOSVOSHk2yb8JjteFaSXdI\n+pvtvYOf/TjJbyY4E4a7W9L2wc7msKQ7JzzPSCb+NhmA7vThEB1ARwgcKIzAgcIIHCiMwIHCCBwo\njMCBwggcKOy/uYiv4U5usicAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Q[:,1].reshape((8,8)),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00413980e-05 3.56566679e-05 9.77713349e-05 1.96404521e-04\n",
      "  3.49589700e-04 6.33728812e-04 2.24116241e-03 2.63676433e-03]\n",
      " [1.66206399e-05 1.80298985e-05 2.33928183e-05 1.12600468e-04\n",
      "  4.41933518e-04 1.30322138e-03 2.05697611e-03 5.79470411e-03]\n",
      " [1.83321817e-05 3.16003106e-05 1.60312645e-05 0.00000000e+00\n",
      "  6.22238641e-04 1.02373922e-03 4.72788911e-03 8.60066885e-03]\n",
      " [2.38676513e-06 1.09060761e-04 2.16780276e-04 2.51420837e-05\n",
      "  0.00000000e+00 0.00000000e+00 1.01178266e-02 1.97932143e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.66045268e-02 8.53567521e-03 4.47150930e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.36709905e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.96875000e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(Q[:,1].reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696318292296302"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "F\u001b[41mH\u001b[0mHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1=ned\n",
    "2=h√∏jre\n",
    "0=venstre\n",
    "3=op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
