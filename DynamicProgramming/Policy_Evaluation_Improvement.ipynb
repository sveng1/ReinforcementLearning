{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy. When we are in state 0, we choose action 0, and when we are in state 1, we choose action 1.\n",
    "pi = {0:0,1:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dynamics\n",
    "p = {}\n",
    "#p[s,a]={(r,s'):P(r,s'|s,a)}\n",
    "p[(0,1)]={(-10,0):1}\n",
    "p[(1,1)]={(-10,0):1}\n",
    "p[(1,0)]={(-1,1):1}\n",
    "p[(0,0)]={(0,0):0.95,(0,1):0.05}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLICY EVALUATION\n",
    "\n",
    "Computing the state-value function for a given policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy,threshold,gamma):\n",
    "    #initialize value function as zeros\n",
    "    V = {i:0 for i in range(len(policy))}\n",
    "    \n",
    "    difference_large = True\n",
    "    while difference_large:\n",
    "        #current value function becomes the old value function\n",
    "        V_old = V.copy()\n",
    "        difference_large = False\n",
    "        \n",
    "        #loop over states\n",
    "        for s in range(len(policy)):\n",
    "            V[s]=0\n",
    "            \n",
    "            #choose action based on policy and state\n",
    "            a = policy[s]\n",
    "            \n",
    "            for reward_and_next_state,prob in p[(s,a)].items():\n",
    "                reward = reward_and_next_state[0]\n",
    "                next_state = reward_and_next_state[1]\n",
    "                \n",
    "                V[s] += prob * (reward + gamma*(V_old[next_state]))\n",
    "            difference = np.abs(V[s] - V_old[s])\n",
    "            difference_large = difference_large or difference>threshold\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -4.219324840306157, 1: -13.78870283073678}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = policy_evaluation(pi,0.01,gamma=0.9)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new value function tells us that being in state 0 and then following $\\pi$ gives an expected discounted return of -4.219324840306157, while being in state 1 and then following $\\pi$ gives an expected discounted return of -13.78870283073678 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLICY IMPROVEMENT\n",
    "\n",
    "Changes the policy based on the value function. In every state, we look for the action that gives the highest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V,gamma,n_actions):\n",
    "    Q = {}\n",
    "    new_policy = {}\n",
    "    \n",
    "    #loop over states\n",
    "    for state in range(len(V)):\n",
    "        best_value = -np.inf\n",
    "        \n",
    "        #loop over actions\n",
    "        for action in range(n_actions):\n",
    "            Q[state,action] = 0\n",
    "            \n",
    "            #loop over possible transitions with rewards\n",
    "            for reward_and_next_state,prob in p[(state,action)].items():\n",
    "                reward = reward_and_next_state[0]\n",
    "                next_state = reward_and_next_state[1]\n",
    "                \n",
    "                #compute value of taking action in state\n",
    "                Q[state,action] += prob * (reward + gamma*V[next_state])\n",
    "            \n",
    "            #if the new state-action value is higher\n",
    "            if Q[state,action] > best_value:\n",
    "                best_value = Q[state,action]\n",
    "                best_action = action\n",
    "        \n",
    "        #assign best action to state\n",
    "        new_policy[state] = best_action\n",
    "    \n",
    "    return new_policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 0, 1: 0},\n",
       " {(0, 0): -3.095875788387243,\n",
       "  (0, 1): -12.785539542518226,\n",
       "  (1, 0): -9.992264459898548,\n",
       "  (1, 1): -12.785539542518226})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pi = policy_improvement(V,0.9,n_actions=2)\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POLICY ITERATION\n",
    "\n",
    "Iterating policy evaluation and policy improvement until stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(policy,gamma,n_actions):\n",
    "    \n",
    "    while True:\n",
    "        V = policy_evaluation(policy=policy,threshold=0.001,gamma=gamma)\n",
    "        \n",
    "        old_policy = policy\n",
    "        policy,Q = policy_improvement(V,gamma=gamma,n_actions=n_actions)\n",
    "        \n",
    "        policy_stable = True\n",
    "        \n",
    "        for state in range(len(V)):\n",
    "            if policy[state] != old_policy[state]:\n",
    "                policy_stable = False\n",
    "                \n",
    "        if policy_stable == True:\n",
    "            return policy, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, Q = policy_iteration(pi,0.9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -3.095875788387243,\n",
       " (0, 1): -12.785539542518226,\n",
       " (1, 0): -9.992264459898548,\n",
       " (1, 1): -12.785539542518226}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
